# GBDT 

重要的理解 


残差只是负梯度的特例，最终的目的是使得损失函数减少的最快，沿哪个方向减少最快呢？负梯度方向。拟合负梯度，相当于损失函数沿着负梯度方向减少。当损失函数为平方损失时，拟合残差就相当于平方损失沿着负梯度方向减小。但是当损失函数不是平方损失时，再拟合残差就不是损失函数沿着负梯度减小了，所以此时当然不能再拟合残差。


您好，我最近看了下boosting相关的一些算法，有几点一直没找到确切的参考资料，在这里想请教一下：
1.GBDT不管是做回归还是分类，是不是都是用一阶导数处的值来近似误差的？
2.GBDT在做二分类时，当损失函数为指数时，很多资料都说退化为Adaboost，但我觉得它俩的中间过程还是有区别的，在这里求证一下；
3.GBDT算法迭代过程中，只是对上一步的误差进行拟合；而Adaboost算法里迭代的每一步都是对样本加权后数据进行拟合，然后使本次误差最小？求证一下
4.Xgboost与GBDT算法本身的主要区别（无论是回归还是分类上，先不考虑性能）：一，是不是Xgboost用二阶导数处的值来近似误差，而GBDT用的是一阶导数呢？ 二，正则化方式不同；三、Xgboost借鉴了随机森林的做法，支持列抽样，来降低过拟合。
5.在Adaboost算法中，体现了对样本加权和对分类器进行加权，在GBDT里是不是没有这两方面的体现？
问题有点多，望回复，谢谢！（实在是比较困惑，专门注册的博客园，前来请教）


1.对的，虽然可以使用不同的损失函数，但是都是用一阶导数处的值来近似误差。
2. 参看108,109,111,113楼的回复。
3. 是这样的。
4. xgboost的确是使用二阶导数，这点和GBDT区别最大。正则化这块xgboost也更灵活。至于支持列抽样，来降低过拟合，现在的GBDT算法比如sklearn的，也已经支持了。
另外，你没有提到的是xgboost还可以支持比较多的其他基学习器。而GBDT一般都还是CART树。
5. GBDT的确和传统的boost算法不一样，不再围绕权重来做文章。它用的是负梯度拟合。那么这个负梯度拟合起到了和Adaboost样本权重类似的作用。

1.GBDT用一阶泰勒展开来近似残差，而XGBOOST用二阶泰勒展开，那么为什么二阶导就更优呢？
2.xgboost算法的参数中有一个subsample，GBDT也有，通过样本子采样来随机化，那么是如何用子采样的样本来训练xgboost或GBDT？比如设置subsample=0.8，然后随机选取80%的样本从第一棵树建到最后一棵树？还是中间某棵树还会随机选择样本？当我看到lightgbm算法的这个参数时更加不懂了，以下是lightgbm算法的参数说明：
bagging_fraction, default=1.0, type=double, 0.0 < bagging_fraction < 1.0, alias=sub_row, subsample
类似于 feature_fraction, 但是它将在不进行重采样的情况下随机选择部分数据
可以用来加速训练
可以用来处理过拟合
Note: 为了启用 bagging, bagging_freq 应该设置为非零值
bagging_freq, default=0, type=int, alias=subsample_freq
bagging 的频率, 0 意味着禁用 bagging. k 意味着每 k 次迭代执行bagging
Note: 为了启用 bagging, bagging_fraction 设置适当


1. 这个要从泰勒展开式来看，一阶泰勒展开式丢弃了二阶以后所有的导数部分，而一阶泰勒展开式丢弃了三阶以后所有的导数部分。可以看出一阶和二阶虽然都是近似，但是二阶的近似准确度会高一些。因为它多保留了展开式的二阶部分。当然，代价是导数计算量变大了。

2. 这里的子采样和随机森林不同，都是不放回采样，这样每颗GBDT,xgBoost树使用的训练样本会有些不同。如果不子采样，则每颗决策树使用的都是全部的训练样本特征。使用了子采样，则每颗决策树使用的都是使用部分(比如80%)的训练样本特征, 虽然都是80%，但每颗树的80%的样本会有些差异。
3. lightGBM的bagging_fraction等价于GBDT,xgBoost的subsample，但是它的功能更加细化，可以只在某些迭代轮使用子采样，其他的迭代轮不使用。这个功能由bagging_freq控制。如果bagging_freq=2， 则只在偶数迭代轮使用子采样。


比如建立第一棵树时用全样本的80%样本，建立第二棵树时用的是剩下的20%样本中随机选取的80%样本？如果是这样的话，第一棵树和第二棵树的样本是没有交集的，但是GBDT和XGBOOST的原理不是每棵树都去拟合每一次迭代的损失么，所以我的理解是同一个样本通过多次迭代拟合损失，最终得到该样本的预测值，但是如果每棵树的样本都不一样，那么每个样本的损失怎么连贯呢？
“是如果每棵树的样本都不一样，那么每个样本的损失怎么连贯呢”， 的确此时损失不连贯，但是这样做虽然训练集的损失可能不够精确，但是模型的泛化能力会变得高一些。专业点的说法就是虽然模型的偏差bias变大了，但是方差variance变小了。